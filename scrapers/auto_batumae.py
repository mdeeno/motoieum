import requests
from bs4 import BeautifulSoup
from supabase import create_client
import os
import time

# 1. í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ í™˜ê²½ë³€ìˆ˜(SUPABASE_URL, SUPABASE_KEY)ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def run_batumae_crawler():
    print("ğŸï¸ [ê°€ë™] ë°”íŠœë§¤ ë¡œë´‡ (Requests ë°©ì‹)")

    # ë„¤ì´ë²„ ì¹´í˜ ì°¨ë‹¨ ë°©ì§€ìš© í—¤ë”
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "Referer": "https://cafe.naver.com/bikecargogo"
    }

    # íƒìƒ‰í•  ê²Œì‹œíŒ URL ë¦¬ìŠ¤íŠ¸ (125cc ì´ìƒ / ë¯¸ë§Œ)
    # iframe ë‚´ë¶€ ì‹¤ì œ ì£¼ì†Œì…ë‹ˆë‹¤.
    urls_to_crawl = [
        # (URL, ë¶„ë¥˜íƒœê·¸)
        ("https://cafe.naver.com/ArticleList.nhn?search.clubid=20188722&search.menuid=1557&search.boardtype=L", "over125"), # 125cc ì´ìƒ
        ("https://cafe.naver.com/ArticleList.nhn?search.clubid=20188722&search.menuid=1558&search.boardtype=L", "under125")  # 125cc ë¯¸ë§Œ
    ]

    total_saved = 0

    for url, tag in urls_to_crawl:
        print(f"\nğŸ” '{tag}' ê²Œì‹œíŒ ìŠ¤ìº” ì¤‘...")
        
        try:
            # 1. í˜ì´ì§€ ì ‘ì†
            res = requests.get(url, headers=headers)
            
            # ì¸ì½”ë”© ì²˜ë¦¬ (ë„¤ì´ë²„ëŠ” ë³´í†µ cp949)
            if "charset=utf-8" in res.text.lower():
                res.encoding = 'utf-8'
            else:
                res.encoding = 'cp949'

            soup = BeautifulSoup(res.text, 'html.parser')

            # 2. ê²Œì‹œê¸€ ëª©ë¡ ì°¾ê¸°
            rows = soup.select('div.article-board > table > tbody > tr')
            
            # ëª©ë¡ì´ ë¹„ì—ˆê±°ë‚˜ ê³µì§€ì‚¬í•­ë§Œ ìˆëŠ” ê²½ìš° ì²´í¬
            if not rows:
                print("   âŒ ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° ë³€ê²½ë¨)")
                continue

            print(f"   -> ê²Œì‹œê¸€ {len(rows)}ê°œ ë°œê²¬! ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")

            for row in rows[:15]: # ìµœëŒ€ 15ê°œë§Œ í™•ì¸
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_tag = row.select_one('a.article')
                    if not title_tag:
                        continue # ê³µì§€ì‚¬í•­ í•„í„°ë§
                        
                    title = title_tag.get_text(strip=True)
                    # href ì•ì— cafe.naver.com ë¶™ì´ê¸°
                    link = "https://cafe.naver.com" + title_tag['href']

                    # ê°€ê²© ì¶”ì¶œ (ë¡œê·¸ì¸ ì•ˆ í•˜ë©´ ì•ˆ ë³´ì¼ ìˆ˜ ìˆìŒ -> 'ê°€ê²©ë¬¸ì˜'ë¡œ ì²˜ë¦¬)
                    price_tag = row.select_one('.td_won')
                    price = price_tag.get_text(strip=True) if price_tag else "ê°€ê²©ë¬¸ì˜"

                    # ë‚ ì§œ/ì‘ì„±ì ë“± (í•„ìš” ì‹œ)
                    date_tag = row.select_one('.td_date')
                    date = date_tag.get_text(strip=True) if date_tag else ""

                    # 3. ì¤‘ë³µ ì²´í¬ (DBì— ì´ë¯¸ ìˆëŠ” ë§í¬ì¸ì§€ í™•ì¸)
                    existing = supabase.table("market").select("id").eq("external_link", link).execute()
                    if existing.data:
                        # ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€
                        print(f"      [Pass] ì¤‘ë³µ: {title}")
                        continue

                    # 4. ì €ì¥í•  ë°ì´í„° êµ¬ì„±
                    data = {
                        "title": f"[ë°”íŠœë§¤] {title}",
                        "content": f"{tag} / {date} ë“±ë¡ (ìƒì„¸ë‚´ìš©ì€ ë§í¬ ì°¸ì¡°)",
                        "price": price,
                        "image_url": "https://cafe.naver.com/favicon.ico", # ë¦¬ìŠ¤íŠ¸ì—ì„  ì´ë¯¸ì§€ ëª» ê°€ì ¸ì˜´
                        "external_link": link,
                        "source": "batumae",
                        "year": "2024",   # ì„ì‹œê°’
                        "mileage": "0km"  # ì„ì‹œê°’
                    }
                    
                    # 5. DB ì €ì¥
                    supabase.table("market").insert(data).execute()
                    print(f"      âœ… [ì €ì¥] {title}")
                    total_saved += 1

                except Exception as e:
                    print(f"      âŒ ì—ëŸ¬: {e}")
                    continue

        except Exception as e:
            print(f"   âŒ ì ‘ì† ì‹¤íŒ¨: {e}")

    print(f"\nğŸ‰ ì´ {total_saved}ê°œì˜ ë°”íŠœë§¤ ë§¤ë¬¼ì„ ìƒˆë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_batumae_crawler()