import os
import time
import shutil
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from supabase import create_client, Client

url: str = "https://qkzrblzjeuowxwkevpfx.supabase.co" 
key: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFrenJibHpqZXVvd3h3a2V2cGZ4Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NTI2OTMwMCwiZXhwIjoyMDgwODQ1MzAwfQ.zdsHm0kIaljLaKM0fCYi_dwVLwMNsDz-__PZTdMvFoo" 

supabase: Client = create_client(url, key)

def run_auto_crawler():
    print("ðŸ¤– [3ì„¸ëŒ€] ë°”íŠœë§¤ ë¡œë´‡ ê°€ë™! (ì „ìš© í”„ë¡œí•„ ëª¨ë“œ)")

    # 1. í˜„ìž¬ í´ë”ì— 'bot_profile'ì´ë¼ëŠ” ë¡œë´‡ ì „ìš© ë°©ì„ ë§Œë“­ë‹ˆë‹¤.
    # ì´ë ‡ê²Œ í•˜ë©´ ë‚´ ì›ëž˜ í¬ë¡¬ê³¼ ì¶©ëŒì´ ì ˆëŒ€ ì•ˆ ë‚©ë‹ˆë‹¤.
    current_folder = os.getcwd()
    profile_path = os.path.join(current_folder, "bot_profile")
    
    print(f"ðŸ“ ë¡œë´‡ í”„ë¡œí•„ ê²½ë¡œ: {profile_path}")

    options = Options()
    # options.add_argument("--headless") 
    
    # ðŸ”¥ [í•µì‹¬] ë¡œë´‡ ì „ìš© í”„ë¡œí•„ ì‚¬ìš©
    options.add_argument(f"user-data-dir={profile_path}")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    
    # ë´‡ íƒì§€ íšŒí”¼
    options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        
        # 2. ë°”íŠœë§¤ ì ‘ì†
        target_url = "https://cafe.naver.com/bikecargogo"
        driver.get(target_url)
        time.sleep(3)

        # ðŸš¨ [ìµœì´ˆ 1íšŒ í•„ìˆ˜] ë¡œê·¸ì¸ì´ ì•ˆ ë˜ì–´ ìžˆë‹¤ë©´ ì‚¬ìš©ìžê°€ ì§ì ‘ ë¡œê·¸ì¸í•˜ê²Œ ê¸°ë‹¤ë ¤ì¤Œ
        if "ë¡œê·¸ì¸" in driver.page_source and "ë¡œê·¸ì•„ì›ƒ" not in driver.page_source:
            print("\n" + "="*60)
            print("ðŸš¨ [ìµœì´ˆ 1íšŒ ì„¤ì •] ë¡œë´‡ ì „ìš© í”„ë¡œí•„ì´ë¼ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤!")
            print("   ì—´ë¦° í¬ë¡¬ ì°½ì—ì„œ 'ë„¤ì´ë²„ ë¡œê·¸ì¸'ì„ ì§ì ‘ í•´ì£¼ì„¸ìš”.")
            print("   ë¡œê·¸ì¸ í›„, ì¹´íŽ˜ ë©”ì¸ í™”ë©´ì´ ë³´ì´ë©´ ì´ í„°ë¯¸ë„ì—ì„œ [Enter]ë¥¼ ì¹˜ì„¸ìš”.")
            print("="*60 + "\n")
            input("âŒ¨ï¸ ë¡œê·¸ì¸ ì™„ë£Œ í›„ ì—”í„° ìž…ë ¥ ëŒ€ê¸° ì¤‘... ")
        
        print("âš¡ í¬ë¡¤ë§ ì‹œìž‘...")

        # iframe ì§„ìž…
        try:
            driver.switch_to.frame("cafe_main")
        except:
            pass

        # ê²Œì‹œê¸€ ê¸ê¸°
        articles = driver.find_elements(By.CSS_SELECTOR, "div.article-board > table > tbody > tr")
        if not articles:
             articles = driver.find_elements(By.CSS_SELECTOR, ".article-board tr")

        print(f"ðŸ“‹ ê²Œì‹œê¸€ {len(articles)}ê°œ ë°œê²¬! ìˆ˜ì§‘ ì‹œìž‘...")
        
        crawled_data = []
        for row in articles[:15]: 
            try:
                title_el = row.find_element(By.CSS_SELECTOR, "a.article")
                title = title_el.text.strip()
                link = title_el.get_attribute("href")
                
                if not any(word in title for word in ["íŒë§¤", "íŒë‹ˆë‹¤", "ê¸‰ë§¤", "ê°€ê²©"]):
                    continue

                price = "ê°€ê²© ë¬¸ì˜"
                import re
                numbers = re.findall(r'\d+', title.replace(',', ''))
                for num in numbers:
                    if len(num) >= 4:
                        val = int(num)
                        if val < 10000: price = str(val * 10000)
                        else: price = str(val)
                        break

                item = {
                    "title": f"[ë°”íŠœë§¤] {title}",
                    "price": price,
                    "location": "ì„œìš¸/ê²½ê¸°",
                    "image_url": "https://cafe.naver.com/favicon.ico",
                    "source": "batumae",
                    "external_link": link,
                    "year": "2023",
                    "mileage": "0",
                    "content": f"ìžë™ ìˆ˜ì§‘ëœ ë§¤ë¬¼ìž…ë‹ˆë‹¤.\n{link}",
                    "status": "íŒë§¤ì¤‘"
                }
                crawled_data.append(item)
                print(f"   Target: {title}")

            except:
                continue

        # DB ì €ìž¥
        if crawled_data:
            print(f"ðŸ’¾ DB ì €ìž¥ ì¤‘... ({len(crawled_data)}ê°œ)")
            for data in crawled_data:
                existing = supabase.table('market').select('id').eq('external_link', data['external_link']).execute()
                if not existing.data:
                    supabase.table('market').insert(data).execute()
            print("ðŸŽ‰ ì €ìž¥ ì™„ë£Œ!")
        else:
            print("ðŸ’¨ ìˆ˜ì§‘ëœ íŒë§¤ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"ðŸ’¥ ì—ëŸ¬ ë°œìƒ: {e}")

    finally:
        if driver:
            driver.quit()

if __name__ == "__main__":
    run_auto_crawler()