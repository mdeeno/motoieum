import requests
from bs4 import BeautifulSoup
from supabase import create_client
import os
import re

# 1. í™˜ê²½ë³€ìˆ˜ ì„¤ì •
SUPABASE_URL = os.environ.get("SUPABASE_URL")
SUPABASE_KEY = os.environ.get("SUPABASE_KEY")

if not SUPABASE_URL or not SUPABASE_KEY:
    print("âŒ í™˜ê²½ë³€ìˆ˜(SUPABASE_URL, SUPABASE_KEY)ê°€ ì—†ìŠµë‹ˆë‹¤.")
    exit(1)

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

def parse_price(price_text):
    if not price_text or "ë¬¸ì˜" in price_text:
        return "ê°€ê²©ë¬¸ì˜"
    # ìˆ«ìë§Œ ë‚¨ê¸°ê³  ì •ì œ
    clean = re.sub(r'[^\d]', '', price_text)
    if not clean:
        return "ê°€ê²©ë¬¸ì˜"
    return clean

def extract_info_from_title(title):
    # ì œëª©ì—ì„œ ì—°ì‹(4ìë¦¬ ìˆ«ì) ì¶”ì¶œ ì‹œë„
    year_match = re.search(r'(20\d{2}|1\d{1})ë…„ì‹', title)
    year = year_match.group(0).replace("ë…„ì‹", "") if year_match else "2024" 
    
    # ì œëª©ì—ì„œ ì£¼í–‰ê±°ë¦¬ ì¶”ì¶œ ì‹œë„
    km_match = re.search(r'(\d+[\d,.]*)(?=km|ë§Œ)', title)
    mileage = km_match.group(0) + "km" if km_match else "0km"
    
    return year, mileage

def run_joongum_crawler():
    print("ğŸ¤– [ê°€ë™] ì¤‘ê²€ë‹¨ ë¡œë´‡ (êµ¬ì¡° ì—…ë°ì´íŠ¸ ë²„ì „)")
    
    url = "http://joongum.co.kr/search_list"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }

    try:
        res = requests.get(url, headers=headers)
        res.encoding = 'utf-8' 

        soup = BeautifulSoup(res.text, 'html.parser')
        
        # âœ… ë³€ê²½ëœ HTML êµ¬ì¡°ì— ëŒ€ì‘í•˜ëŠ” ë©€í‹° ì„ íƒì
        items = soup.select('.products_list > li') or \
                soup.select('.list_box > li') or \
                soup.select('.product_item')

        if not items:
            print("âŒ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (HTML êµ¬ì¡° ë³€ê²½ë¨)")
            return

        print(f"   -> ë§¤ë¬¼ {len(items)}ê°œ ë°œê²¬! ë¶„ì„ ì‹œì‘...")

        count = 0
        for item in items[:15]: 
            try:
                link_tag = item.select_one('a')
                if not link_tag: continue
                
                href = link_tag.get('href')
                link = f"http://joongum.co.kr{href}" if not href.startswith('http') else href
                
                title_tag = item.select_one('.title') or item.select_one('.subject')
                if not title_tag: continue
                title = title_tag.get_text(strip=True)

                # ì—°ì‹/ì£¼í–‰ê±°ë¦¬ ì¶”ì¶œ ë¡œì§ ìœ ì§€
                year, mileage = extract_info_from_title(title)

                price_tag = item.select_one('.price')
                raw_price = price_tag.get_text(strip=True) if price_tag else "0"
                price = parse_price(raw_price)

                img_tag = item.select_one('img')
                img_src = img_tag.get('src') if img_tag else None
                if img_src and not img_src.startswith('http'):
                    img_src = f"http://joongum.co.kr{img_src}"

                # ì¤‘ë³µ ì²´í¬
                existing = supabase.table("market").select("id").eq("external_link", link).execute()
                if existing.data:
                    continue

                data = {
                    "title": f"[ì¤‘ê²€ë‹¨] {title}",
                    "content": "ì¤‘ê²€ë‹¨ ë§¤ë¬¼ì…ë‹ˆë‹¤. ìƒì„¸ ë‚´ìš©ì€ ë§í¬ë¥¼ í™•ì¸í•˜ì„¸ìš”.", 
                    "price": price,
                    "year": year,
                    "mileage": mileage,
                    "image_url": img_src,
                    "external_link": link,
                    "source": "joongum"
                }

                supabase.table("market").insert(data).execute()
                print(f"      âœ… [ì €ì¥] {title}")
                count += 1

            except Exception as e:
                print(f"      âš ï¸ í•­ëª© ì²˜ë¦¬ ì¤‘ ì—ëŸ¬: {e}")
                continue

        print(f"\nğŸ‰ ì´ {count}ê°œì˜ ì¤‘ê²€ë‹¨ ë§¤ë¬¼ì„ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âŒ í¬ë¡¤ë§ ì „ì²´ ì—ëŸ¬: {e}")

if __name__ == "__main__":
    run_joongum_crawler()