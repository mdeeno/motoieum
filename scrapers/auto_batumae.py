import os
import time
import re
from dotenv import load_dotenv
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from supabase import create_client, Client

# --- ê¸°ë³¸ ì„¤ì • ---
load_dotenv()
url = os.getenv("SUPABASE_URL")
key = os.getenv("SUPABASE_KEY")

if not url or not key:
    raise ValueError("âŒ .env íŒŒì¼ì´ ì—†ê±°ë‚˜ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")

supabase: Client = create_client(url, key)

# --- ë°ì´í„° ì •ì œ í•¨ìˆ˜ë“¤ ---
def clean_number(text):
    return re.sub(r'[^\d]', '', text)

def parse_price_line(text):
    if not text: return "ê°€ê²© ë¬¸ì˜"
    # ê°€ê²© íŒŒì‹± ì‹œ km, cc ë“±ì€ ì œê±°
    clean_text = re.sub(r'[\d,.]+\s*(km|í‚¤ë¡œ|k|m|cc)', '', text.lower())
    
    man_match = re.search(r'(\d+)[\s,]*[ë§Œ]', clean_text)
    if man_match:
        val = int(clean_number(man_match.group(1)))
        return str(val * 10000)
        
    nums = re.findall(r'\d+', clean_text.replace(',', ''))
    if nums:
        best_num = max(nums, key=len)
        val = int(best_num)
        # ì—°ì‹(1990~2026)ì´ë‚˜ ë°°ê¸°ëŸ‰(50~1000cc)ì´ë‘ í—·ê°ˆë¦¬ì§€ ì•Šê²Œ ì²˜ë¦¬
        if 1990 <= val <= 2026: return "ê°€ê²© ë¬¸ì˜"
        if 50 <= val < 10000: return str(val * 10000)
        elif val >= 10000: return str(val)
    return "ê°€ê²© ë¬¸ì˜"

def parse_mileage_line(text):
    """ 
    ê°œì„ ì : 
    1. 5@@@@ -> 50000 ë³€í™˜
    2. 1,390,000 ê°™ì€ ê°€ê²© ë°ì´í„°ë¥¼ ì£¼í–‰ê±°ë¦¬ë¡œ ì°©ê°í•˜ì§€ ì•Šê²Œ 'ìƒí•œì„ (Max Limit)' ë„ì… 
    """
    if not text: return "0"
    
    # 1. íŠ¹ìˆ˜ë¬¸ì(@, x, ?)ë¥¼ 0ìœ¼ë¡œ ì¹˜í™˜
    text = text.lower().replace('x', '0').replace('@', '0').replace('?', '0')
    
    # 2. 'ë§Œ' + 'ì²œ' ë³µí•© íŒ¨í„´
    if 'ë§Œ' in text and 'ì²œ' in text:
        match = re.search(r'(\d+)\s*ë§Œ\s*(\d+)\s*ì²œ', text)
        if match:
            try:
                val = (int(match.group(1)) * 10000) + (int(match.group(2)) * 1000)
                return f"{val:,}"
            except: pass

    # 3. 'ë§Œ' ë‹¨ìœ„
    if 'ë§Œ' in text:
        man_match = re.search(r'([\d.]+)\s*ë§Œ', text)
        if man_match:
            try: return f"{int(float(man_match.group(1)) * 10000):,}"
            except: pass
            
    # 4. 'ì²œ' ë‹¨ìœ„
    if 'ì²œ' in text:
        chun_match = re.search(r'([\d.]+)\s*ì²œ', text)
        if chun_match:
            try: return f"{int(float(chun_match.group(1)) * 1000):,}"
            except: pass

    # 5. ì¼ë°˜ ìˆ«ì (ì½¤ë§ˆ, ì  ì œê±°)
    clean_text_for_num = text.replace(',', '').replace('.', '')
    nums = re.findall(r'\d+', clean_text_for_num)
    
    if nums:
        best_num = max(nums, key=len)
        try:
            val = int(best_num)
            # [í•µì‹¬] ì•ˆì „ì¥ì¹˜: ì£¼í–‰ê±°ë¦¬ê°€ 50ë§Œkm ë„˜ìœ¼ë©´ ê°€ê²©ìœ¼ë¡œ ì˜¤ì¸í•œ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ì—¬ ë¬´ì‹œ
            if val > 500000: 
                return "0"
            return f"{val:,}"
        except: pass
        
    return "0"

def parse_year(text):
    if not text: return "ì—°ì‹ ë¯¸ìƒ"
    text = text.replace(' ', '')
    match = re.search(r'(20\d{2})', text)
    if match:
        year = int(match.group(1))
        if 1990 <= year <= 2026: return str(year)
    match = re.search(r'(\d{2})[\s]*(ë…„|ë…„ì‹)', text)
    if match: return f"20{match.group(1)}"
    return "ì—°ì‹ ë¯¸ìƒ"

def analyze_content(title, content_text):
    lines = content_text.split('\n')
    price = "ê°€ê²© ë¬¸ì˜"
    year = "ì—°ì‹ ë¯¸ìƒ"
    mileage = "0"
    
    found_year = False
    found_mileage = False
    found_price = False

    # 1ì°¨ ìŠ¤ìº”: ì§€ì •ëœ ì¤„ (3, 4, 6ë²ˆ)
    for line in lines:
        clean_line = line.strip().replace(' ', '')
        
        # 3. ì œì‘ì—°ì‹
        if not found_year and ('3.ì œì‘ì—°ì‹' in clean_line or '3.ì—°ì‹' in clean_line):
            val = line.split(':')[-1].strip()
            parsed = parse_year(val)
            if parsed != "ì—°ì‹ ë¯¸ìƒ": 
                year = parsed
                found_year = True
        
        # 4. ì ì‚°ê±°ë¦¬
        if not found_mileage and ('4.ì ì‚°ê±°ë¦¬' in clean_line):
            val = line.split(':')[-1].strip()
            parsed = parse_mileage_line(val)
            # 0ì´ ì•„ë‹ˆì–´ì•¼ ì±„íƒ (5@@@@ê°€ 50000ìœ¼ë¡œ ì˜ ë³€í™˜ë˜ë©´ ì—¬ê¸°ì„œ ì±„íƒë¨)
            if parsed != "0": 
                mileage = parsed
                found_mileage = True

        # 6. íŒë§¤ í¬ë§ê°€ê²©
        if not found_price and ('6.íŒë§¤í¬ë§ê°€ê²©' in clean_line or '6.í¬ë§ê°€ê²©' in clean_line):
            val = line.split(':')[-1].strip()
            parsed = parse_price_line(val)
            if parsed != "ê°€ê²© ë¬¸ì˜": 
                price = parsed
                found_price = True

    # 2ì°¨ ìŠ¤ìº” (ë°±ì—…): ì§€ì •ëœ ì¤„ì—ì„œ ëª» ì°¾ì•˜ì„ ë•Œ
    if not found_price:
        p_from_title = parse_price_line(title)
        if p_from_title != "ê°€ê²© ë¬¸ì˜": price = p_from_title
        else:
            for line in lines:
                if any(k in line for k in ['íŒë§¤ê¸ˆì•¡', 'í¬ë§ê°€ê²©', 'ê°€ê²©']):
                    p = parse_price_line(line.split(':')[-1])
                    if p != "ê°€ê²© ë¬¸ì˜": 
                        price = p
                        break
                        
    if not found_year:
        y_from_title = parse_year(title)
        if y_from_title != "ì—°ì‹ ë¯¸ìƒ": year = y_from_title

    if not found_mileage:
        # ì œëª©ì—ì„œ km ì°¾ê¸°
        if 'km' in title.lower() or 'í‚¤ë¡œ' in title:
            m = parse_mileage_line(title)
            if m != "0": mileage = m
            
        # ë³¸ë¬¸ ì „ì²´ ë’¤ì§€ê¸°
        if mileage == "0":
            for line in lines:
                # [í•µì‹¬] ê°€ê²© ê´€ë ¨ëœ ì¤„ì€ ì£¼í–‰ê±°ë¦¬ ì°¾ì„ ë•Œ ë¬´ì‹œ (1390000 ë°©ì§€)
                if any(bad_word in line for bad_word in ['ê°€ê²©', 'ë§Œì›', 'ì›', 'ê¸ˆì•¡']):
                    continue
                    
                if any(k in line for k in ['ì ì‚°ê±°ë¦¬', 'ì£¼í–‰ê±°ë¦¬', 'í‚¤ë¡œìˆ˜']):
                    m = parse_mileage_line(line.split(':')[-1])
                    if m != "0": 
                        mileage = m
                        break
    return price, year, mileage

# --- ì´í•˜ í¬ë¡¤ë§ ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼ ---
def ensure_content_loaded(driver):
    try:
        driver.find_element(By.CSS_SELECTOR, "a.article, a.tit, a.m-tcol-c, a.bit")
        return True
    except: pass
    try:
        driver.switch_to.frame("cafe_main")
        driver.find_element(By.CSS_SELECTOR, "a.article, a.tit, a.m-tcol-c, a.bit")
        return True
    except:
        driver.switch_to.default_content()
        return False

def scrape_board_final(driver, keyword, category_name):
    print(f"\n   ğŸ” '{keyword}' ì²˜ë¦¬ ì‹œì‘...")
    driver.switch_to.default_content()
    
    target_url = None
    try:
        xpath = f"//a[contains(text(), '{keyword}')]"
        links = driver.find_elements(By.XPATH, xpath)
        if links: target_url = links[0].get_attribute("href")
    except: pass

    if not target_url: return []

    driver.get(target_url)
    time.sleep(2)
    
    if not ensure_content_loaded(driver): return []

    try:
        link_elements = driver.find_elements(By.CSS_SELECTOR, "a.article, a.tit, a.m-tcol-c, a.bit")
    except: return []

    links = list(set([el.get_attribute("href") for el in link_elements if el.text.strip()]))
    target_links = links[:12]

    collected = []
    for i, link in enumerate(target_links):
        try:
            print(f"      [{i+1}/{len(target_links)}] ë¶„ì„ ì¤‘...", end="\r")
            driver.get(link)
            time.sleep(0.8)

            driver.switch_to.default_content()
            try:
                title = driver.find_element(By.CSS_SELECTOR, "h3.title_text").text.strip()
            except:
                try:
                    driver.switch_to.frame("cafe_main")
                    title = driver.find_element(By.CSS_SELECTOR, "h3.title_text").text.strip()
                except: continue

            if not any(word in title for word in ["íŒë§¤", "íŒë‹ˆë‹¤", "ê¸‰ë§¤", "ê°€ê²©"]): continue

            content_text = ""
            try:
                content_el = driver.find_element(By.CSS_SELECTOR, "div.se-main-container, div.ContentRenderer")
                content_text = content_el.text
            except: pass

            img_src = "https://cafe.naver.com/favicon.ico"
            try:
                images = driver.find_elements(By.CSS_SELECTOR, "div.se-module-image img, div.se-image-resource")
                for img in images:
                    src = img.get_attribute("src")
                    if src and not any(x in src for x in ["sticker", "emoji", "profile"]):
                        if float(img.get_attribute("width") or 0) > 300:
                            img_src = src
                            break
            except: pass

            price, year, mileage = analyze_content(title, content_text)

            data = {
                "title": f"[ë°”íŠœë§¤] {title}",
                "price": price,
                "location": "ì„œìš¸/ê²½ê¸°",
                "image_url": img_src,
                "source": "batumae",
                "external_link": link,
                "year": year, 
                "mileage": mileage,
                "content": f"#{category_name} \n{content_text[:100]}...",
                "status": "íŒë§¤ì¤‘"
            }
            collected.append(data)
        except Exception: continue
            
    return collected

def run_auto_crawler():
    print("ğŸ¤– [ìµœì¢…ë³¸] ë°”íŠœë§¤ ë¡œë´‡ (ì£¼í–‰ê±°ë¦¬ 50ë§Œkm ì´ˆê³¼ ì‹œ ë¬´ì‹œ)")
    
    current_folder = os.getcwd()
    profile_path = os.path.join(current_folder, "bot_profile")
    
    options = Options()
    options.add_argument(f"user-data-dir={profile_path}")
    options.add_argument("--start-maximized")
    options.add_argument("--headless")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-popup-blocking")
    
    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get("https://cafe.naver.com/bikecargogo")
        time.sleep(3)

        total_data = []
        total_data.extend(scrape_board_final(driver, "125ccì´ˆê³¼", "over125"))
        total_data.extend(scrape_board_final(driver, "125ccë¯¸ë§Œ", "under125"))

        if total_data:
            print(f"\nğŸ’¾ ì´ {len(total_data)}ê°œ ë°ì´í„° ì €ì¥/ê°±ì‹  ì¤‘...")
            for data in total_data:
                existing = supabase.table('market').select('id').eq('external_link', data['external_link']).execute()
                if not existing.data:
                    supabase.table('market').insert(data).execute()
                else:
                    supabase.table('market').update({
                        "image_url": data["image_url"],
                        "price": data["price"],
                        "year": data["year"],
                        "mileage": data["mileage"]
                    }).eq('external_link', data['external_link']).execute()
            print("ğŸ‰ ì™„ë£Œ!")
        else:
            print("ğŸ’¨ ë°ì´í„° ì—†ìŒ")

    except Exception as e:
        print(f"ğŸ’¥ ì—ëŸ¬: {e}")
    finally:
        if driver: driver.quit()

if __name__ == "__main__":
    run_auto_crawler()