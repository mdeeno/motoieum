import os
import time
import re
# ğŸ‘‡ ì´ê±° ê¼­ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤
from dotenv import load_dotenv 
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from supabase import create_client, Client

# ğŸ‘‡ .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# ğŸ‘‡ ì½”ë“œì— ë¹„ë°€ë²ˆí˜¸ ì§ì ‘ ì ì§€ ì•Šê³  ê°€ì ¸ì˜¤ê¸°
url = os.getenv("SUPABASE_URL")
key = os.getenv("SUPABASE_KEY")

# ì•ˆì „ì¥ì¹˜
if not url or not key:
    raise ValueError("âŒ .env íŒŒì¼ì´ ì—†ê±°ë‚˜ í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")

supabase: Client = create_client(url, key)
def clean_number(text):
    return re.sub(r'[^\d]', '', text)

def parse_price_line(text):
    clean_text = re.sub(r'[\d,.]+\s*(km|í‚¤ë¡œ|k|m|cc)', '', text.lower())
    man_match = re.search(r'(\d+)[\s,]*[ë§Œ]', clean_text)
    if man_match:
        val = int(clean_number(man_match.group(1)))
        return str(val * 10000)
    nums = re.findall(r'\d+', clean_text.replace(',', ''))
    if nums:
        best_num = max(nums, key=len)
        val = int(best_num)
        if 50 <= val < 10000: return str(val * 10000)
        elif val >= 10000: return str(val)
    return "ê°€ê²© ë¬¸ì˜"

def parse_mileage_line(text):
    text = text.lower().replace('x', '0').replace('@', '0').replace('?', '0')
    if 'ë§Œ' in text:
        man_match = re.search(r'([\d.]+)\s*ë§Œ', text)
        if man_match:
            try:
                val = float(man_match.group(1)) * 10000
                return f"{int(val):,}"
            except: pass
    nums = re.findall(r'\d+', text.replace(',', ''))
    if nums:
        best_num = max(nums, key=len)
        try:
            return f"{int(best_num):,}"
        except: pass
    return "0"

def parse_year(text):
    match = re.search(r'(20\d{2})', text)
    if match:
        year = match.group(1)
        if 1990 <= int(year) <= 2026: return year
    match = re.search(r'(\d{2})\s*(ë…„|ë…„ì‹)', text)
    if match:
        yy = match.group(1)
        return f"20{yy}"
    return "ì—°ì‹ ë¯¸ìƒ"

def analyze_content(title, content_text):
    lines = content_text.split('\n') 
    price = "ê°€ê²© ë¬¸ì˜"
    year = "ì—°ì‹ ë¯¸ìƒ"
    mileage = "0"

    for line in lines:
        clean_line = line.strip().replace(' ', '')
        if any(k in clean_line for k in ['íŒë§¤ê¸ˆì•¡', 'í¬ë§ê°€ê²©', 'íŒë§¤ê°€ê²©', 'ê°€ê²©']):
            parts = line.split(':')
            target = parts[-1] if len(parts) > 1 else line
            p = parse_price_line(target)
            if p != "ê°€ê²© ë¬¸ì˜": price = p
        if any(k in clean_line for k in ['ì—°ì‹', 'ë…„ì‹', 'ì œì‘']):
            y = parse_year(line)
            if y != "ì—°ì‹ ë¯¸ìƒ": year = y
        if any(k in clean_line for k in ['ì ì‚°ê±°ë¦¬', 'ì£¼í–‰ê±°ë¦¬', 'í‚¤ë¡œìˆ˜', 'ì‹¤í‚¤ë¡œìˆ˜']):
            parts = line.split(':')
            target = parts[-1] if len(parts) > 1 else line
            m = parse_mileage_line(target)
            if m != "0": mileage = m

    if price == "ê°€ê²© ë¬¸ì˜":
        p = parse_price_line(title) 
        if p != "ê°€ê²© ë¬¸ì˜": price = p
    if year == "ì—°ì‹ ë¯¸ìƒ": year = parse_year(title)
    if mileage == "0" and ('km' in title.lower() or 'í‚¤ë¡œ' in title):
        m = parse_mileage_line(title)
        if m != "0": mileage = m
    return price, year, mileage

def scrape_board_auto(driver, category_name):
    print(f"   âš¡ '{category_name}' ë¶„ì„ ì‹œì‘...")
    try:
        driver.switch_to.default_content()
        driver.switch_to.frame("cafe_main")
    except: pass

    link_elements = driver.find_elements(By.CSS_SELECTOR, "a.article, a.tit, a.m-tcol-c")
    links = list(set([el.get_attribute("href") for el in link_elements if el.text.strip()]))
    target_links = links[:15]

    collected = []
    for i, link in enumerate(target_links):
        try:
            print(f"      [{i+1}/{len(target_links)}] ë¶„ì„ ì¤‘...", end="\r")
            driver.get(link)
            time.sleep(1)

            try:
                driver.switch_to.default_content()
                driver.switch_to.frame("cafe_main")
            except: pass

            try:
                title = driver.find_element(By.CSS_SELECTOR, "h3.title_text").text.strip()
            except: continue

            if not any(word in title for word in ["íŒë§¤", "íŒë‹ˆë‹¤", "ê¸‰ë§¤", "ê°€ê²©"]): continue

            content_text = ""
            try:
                content_el = driver.find_element(By.CSS_SELECTOR, "div.se-main-container, div.ContentRenderer")
                content_text = content_el.text
            except: pass

            img_src = None
            try:
                images = driver.find_elements(By.CSS_SELECTOR, "div.se-module-image img, div.se-image-resource, div.ContentRenderer img")
                for img in images:
                    src = img.get_attribute("src")
                    if not src: continue
                    if any(x in src for x in ["sticker", "emoji", "profile", "blank", "transparent"]): continue
                    width = float(img.get_attribute("width") or 0)
                    if width > 300 or "postfiles.pstatic.net" in src:
                        img_src = src
                        break
            except: pass
            if not img_src: img_src = "https://cafe.naver.com/favicon.ico"

            price, year, mileage = analyze_content(title, content_text)

            data = {
                "title": f"[ë°”íŠœë§¤] {title}",
                "price": price,
                "location": "ì„œìš¸/ê²½ê¸°",
                "image_url": img_src,
                "source": "batumae",
                "external_link": link,
                "year": year, 
                "mileage": mileage,
                "content": f"#{category_name} #ë°”íŠœë§¤ \n{content_text[:100]}...",
                "status": "íŒë§¤ì¤‘"
            }
            collected.append(data)
            print(f"      âœ… [{year} / {mileage}km / {price}] {title[:8]}...    ")
        except: continue
    return collected

def run_auto_crawler():
    print("ğŸ¤– [18ì„¸ëŒ€] ë°”íŠœë§¤ ë¡œë´‡ (ID ì¶”ì  í´ë¦­ ë°©ì‹)")
    
    current_folder = os.getcwd()
    profile_path = os.path.join(current_folder, "bot_profile")
    
    options = Options()
    options.add_argument(f"user-data-dir={profile_path}")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    options.add_experimental_option("excludeSwitches", ["enable-automation"])
    options.add_experimental_option('useAutomationExtension', False)

    driver = None
    try:
        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)
        driver.get("https://cafe.naver.com/bikecargogo")
        time.sleep(3) 

        total_data = []

        # ğŸš€ [1ë‹¨ê³„] 125cc ì´ˆê³¼ (menuid=77 ì°¾ì•„ì„œ í´ë¦­)
        print("\nğŸš€ [1ë‹¨ê³„] 125cc ì´ˆê³¼ ê²Œì‹œíŒ ID(77) ì°¾ëŠ” ì¤‘...")
        try:
            # ë©”ë‰´ê°€ ì™¼ìª½ì— ìˆìœ¼ë¯€ë¡œ ë©”ì¸ í”„ë ˆì„ìœ¼ë¡œ ë‚˜ê°€ì•¼ í•¨
            driver.switch_to.default_content()
            # href ì†ì„±ì— 'menuid=77'ì´ í¬í•¨ëœ a íƒœê·¸ ì°¾ê¸°
            menu = driver.find_element(By.CSS_SELECTOR, "a[href*='menuid=77']")
            menu.click()
            time.sleep(2)
            total_data.extend(scrape_board_auto(driver, "over125"))
        except Exception as e:
            print(f"   âš ï¸ ë©”ë‰´ í´ë¦­ ì‹¤íŒ¨: {e}")

        # ğŸ›µ [2ë‹¨ê³„] 125cc ë¯¸ë§Œ (menuid=78 ì°¾ì•„ì„œ í´ë¦­)
        print("\nğŸ›µ [2ë‹¨ê³„] 125cc ë¯¸ë§Œ ê²Œì‹œíŒ ID(78) ì°¾ëŠ” ì¤‘...")
        try:
            driver.switch_to.default_content() # ë‹¤ì‹œ ë©”ì¸ìœ¼ë¡œ
            # href ì†ì„±ì— 'menuid=78'ì´ í¬í•¨ëœ a íƒœê·¸ ì°¾ê¸°
            menu = driver.find_element(By.CSS_SELECTOR, "a[href*='menuid=78']")
            menu.click()
            time.sleep(2)
            total_data.extend(scrape_board_auto(driver, "under125"))
        except Exception as e:
            print(f"   âš ï¸ ë©”ë‰´ í´ë¦­ ì‹¤íŒ¨: {e}")

        if total_data:
            print(f"\nğŸ’¾ ì´ {len(total_data)}ê°œ ë°ì´í„° ì €ì¥ ì¤‘...")
            for data in total_data:
                existing = supabase.table('market').select('id').eq('external_link', data['external_link']).execute()
                if not existing.data:
                    supabase.table('market').insert(data).execute()
                else:
                     supabase.table('market').update({
                         "image_url": data["image_url"],
                         "price": data["price"],
                         "year": data["year"],
                         "mileage": data["mileage"],
                         "content": data["content"]
                     }).eq('external_link', data['external_link']).execute()
            print("ğŸ‰ ìˆ˜ì§‘ ì™„ë£Œ!")
        else:
            print("ğŸ’¨ ë°ì´í„° ì—†ìŒ")

    except Exception as e:
        print(f"ğŸ’¥ ì—ëŸ¬ ë°œìƒ: {e}")

    finally:
        if driver: driver.quit()

if __name__ == "__main__":
    run_auto_crawler()