import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from supabase import create_client, Client

# ğŸ‘‡ [í•„ìˆ˜] ë³¸ì¸ì˜ Supabase URLê³¼ Service Role Keyë¥¼ ë„£ìœ¼ì„¸ìš”!
url: str = "https://qkzrblzjeuowxwkevpfx.supabase.co"
key: str = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InFrenJibHpqZXVvd3h3a2V2cGZ4Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc2NTI2OTMwMCwiZXhwIjoyMDgwODQ1MzAwfQ.zdsHm0kIaljLaKM0fCYi_dwVLwMNsDz-__PZTdMvFoo" 

supabase: Client = create_client(url, key)

def get_real_data():
    print("ğŸ”¥ [ì‹¤ì „] ë°”íŠœë§¤ ì ‘ì† ì‹œë„ ì¤‘... (í¬ë¡¬ ì°½ì´ ì—´ë¦½ë‹ˆë‹¤)")

    # 1. í¬ë¡¬ ë¸Œë¼ìš°ì € ì˜µì…˜ ì„¤ì • (ì‚¬ëŒì²˜ëŸ¼ ë³´ì´ê²Œ í•˜ê¸°)
    chrome_options = Options()
    # chrome_options.add_argument("--headless") # ì°½ ì—†ì´ ì‹¤í–‰í•˜ë ¤ë©´ ì£¼ì„ í•´ì œ (í…ŒìŠ¤íŠ¸ ë• ì°½ ë³´ëŠ”ê²Œ ì¢‹ìŒ)
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    # ë„¤ì´ë²„ê°€ ë´‡ì„ ì°¨ë‹¨í•˜ì§€ ì•Šë„ë¡ User-Agent ì„¤ì •
    chrome_options.add_argument("user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")

    # 2. ë¸Œë¼ìš°ì € ì‹¤í–‰
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    
    # ë°”íŠœë§¤ 'ì˜¤í† ë°”ì´ íŒë‹ˆë‹¤' ê²Œì‹œíŒ URL (ì „ì²´ê¸€ë³´ê¸° ì˜ˆì‹œ)
    # ì‹¤ì œë¡œëŠ” íŠ¹ì • ì¹´í…Œê³ ë¦¬ IDê°€ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„  ì˜ˆì‹œë¡œ ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•˜ê±°ë‚˜ íŠ¹ì • ê²Œì‹œíŒì„ ê°‘ë‹ˆë‹¤.
    target_url = "https://cafe.naver.com/bikecargogo?iframe_url=/ArticleList.nhn%3Fsearch.clubid=10076228%26search.menuid=79%26search.boardtype=L"
    
    try:
        driver.get(target_url)
        time.sleep(3) # ë¡œë”© ëŒ€ê¸° (í•„ìˆ˜)

        # ğŸš¨ [í•µì‹¬] ë„¤ì´ë²„ ì¹´í˜ëŠ” 'cafe_main'ì´ë¼ëŠ” iframe ì•ˆì— ë‚´ìš©ì´ ìˆìŠµë‹ˆë‹¤. ê±°ê¸°ë¡œ ë“¤ì–´ê°€ì•¼ í•¨.
        driver.switch_to.frame("cafe_main")

        # 3. ê²Œì‹œê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        articles = driver.find_elements(By.CSS_SELECTOR, "div.article-board > table > tbody > tr")
        
        crawled_data = []
        print(f"ğŸ“‹ ê²Œì‹œê¸€ {len(articles)}ê°œ ë°œê²¬! ë°ì´í„° ì¶”ì¶œ ì‹œì‘...")

        for row in articles[:10]: # í…ŒìŠ¤íŠ¸ë¡œ ìƒìœ„ 10ê°œë§Œ ê°€ì ¸ì˜µë‹ˆë‹¤.
            try:
                # ì œëª© ê°€ì ¸ì˜¤ê¸°
                title_element = row.find_element(By.CSS_SELECTOR, "a.article")
                title = title_element.text.strip()
                link = title_element.get_attribute("href")
                
                # ì‘ì„±ì / ë‚ ì§œ ë“± (í•„ìš”ì‹œ ì¶”ê°€)
                
                # ê°€ê²©ì´ë‚˜ ì—°ì‹ì€ ì œëª©ì—ì„œ ì¶”ì¸¡í•˜ê±°ë‚˜ ë“¤ì–´ê°€ë´ì•¼ ì•Œ ìˆ˜ ìˆìŒ.
                # ì¼ë‹¨ ì œëª©ì— ê°€ê²©ì´ ì—†ìœ¼ë©´ 'ê°€ê²©ë¬¸ì˜'ë¡œ ì²˜ë¦¬í•˜ê±°ë‚˜ ì„ì˜ê°’ ì„¤ì •
                # (ì‹¤ì œë¡œëŠ” ìƒì„¸í˜ì´ì§€ ë“¤ì–´ê°€ì„œ íŒŒì‹±í•´ì•¼ ì •í™•í•¨ -> ì†ë„ ëŠë ¤ì§)
                
                # ì¸ë„¤ì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ë¦¬ìŠ¤íŠ¸í˜•ì—” ì—†ì„ ìˆ˜ë„ ìˆìŒ)
                image_url = "https://via.placeholder.com/300?text=No+Image" # ê¸°ë³¸ ì´ë¯¸ì§€

                if "íŒë§¤" in title or "íŒë‹ˆë‹¤" in title: # íŒë§¤ê¸€ë§Œ í•„í„°ë§
                    print(f"   - ìˆ˜ì§‘ ì¤‘: {title}")
                    
                    item = {
                        "title": title,
                        "price": "ê°€ê²© ë¬¸ì˜", # ìƒì„¸ í˜ì´ì§€ ì•ˆë“¤ì–´ê°€ë©´ ì•Œê¸° í˜ë“¦ (ì¶”í›„ ê³ ë„í™” ê°€ëŠ¥)
                        "location": "ì „êµ­",   # ìƒì„¸ í˜ì´ì§€ ì•ˆë“¤ì–´ê°€ë©´ ì•Œê¸° í˜ë“¦
                        "image_url": image_url,
                        "source": "batumae",
                        "external_link": link,
                        "year": "2023",      # ì œëª© íŒŒì‹± í•„ìš” (ì¼ë‹¨ ê¸°ë³¸ê°’)
                        "mileage": "0",
                        "content": f"ë°”íŠœë§¤ ì‹¤ì‹œê°„ ë§¤ë¬¼ì…ë‹ˆë‹¤.\nì›ë¬¸ ë§í¬ë¥¼ í´ë¦­í•˜ì„¸ìš”: {link}",
                        "status": "íŒë§¤ì¤‘"
                    }
                    crawled_data.append(item)
            
            except Exception as e:
                continue # ì—ëŸ¬ë‚˜ë©´ ë‹¤ìŒ ê¸€

        # 4. DBì— ì €ì¥
        if crawled_data:
            print(f"ğŸ’¾ ì´ {len(crawled_data)}ê°œì˜ ë§¤ë¬¼ì„ DBì— ì €ì¥í•©ë‹ˆë‹¤...")
            for data in crawled_data:
                # ì œëª©ìœ¼ë¡œ ì¤‘ë³µ ì²´í¬ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
                existing = supabase.table('market').select('id').eq('external_link', data['external_link']).execute()
                if not existing.data:
                    supabase.table('market').insert(data).execute()
                    print(f"   âœ… ì €ì¥ ì™„ë£Œ: {data['title']}")
                else:
                    print(f"   âš ï¸ ì´ë¯¸ ì¡´ì¬í•¨: {data['title']}")
        else:
            print("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë„¤ì´ë²„ êµ¬ì¡°ê°€ ë°”ë€Œì—ˆê±°ë‚˜ ì°¨ë‹¨ë¨)")

    except Exception as e:
        print(f"ğŸš« ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
    
    finally:
        driver.quit() # ë¸Œë¼ìš°ì € ë„ê¸°
        print("ğŸ í¬ë¡¤ë§ ì¢…ë£Œ")

if __name__ == "__main__":
    get_real_data()